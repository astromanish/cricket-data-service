{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### config ############\n",
    "delta_lake_path = '../CricketDataService_New/deltalake/main/players' #local\n",
    "# delta_lake_path = 'az://sports/final_players_data' #azure\n",
    "options={\"AZURE_STORAGE_ACCOUNT_NAME\":\"jioaishared\",\n",
    " \"AZURE_STORAGE_ACCOUNT_KEY\":\"OYRIjKqneeiI+UrDyLHV7CBOudfdxb73AqkdtUYX3UGYVynUDjQdCtv60a73oEf++qIYnY6844QUOSPrcndLuQ==\"}\n",
    "init_data_path = '../Data/Squad/init/'\n",
    "test_data_path = '../Data/Squad/test/'\n",
    "main_data_path = '../Data/Squad/main/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ packages ###########\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow as pa \n",
    "from deltalake import DeltaTable\n",
    "from deltalake.writer import write_deltalake\n",
    "from azure.storage.filedatalake import DataLakeFileClient\n",
    "from azure.storage.blob import BlobServiceClient, generate_account_sas, ResourceTypes, AccountSasPermissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ utils ###########\n",
    "\n",
    "def convert_dataframe_to_json_list(dataframe):\n",
    "    data_dict = dataframe.to_dict(orient='records')\n",
    "    json_list = [json.dumps(data) for data in data_dict]\n",
    "    return json_list\n",
    "def convert_id_to_integer(id_string, range_size):\n",
    "    id_hash = hash(id_string)  \n",
    "    id_integer = abs(id_hash) % range_size  \n",
    "    return id_integer\n",
    "def read_and_parse_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "    content = content.strip()[8:-2]\n",
    "    content = json.loads(content)\n",
    "    return content\n",
    "def read_json_files(folder_path):\n",
    "    data_list = []\n",
    "    for path in Path(folder_path).rglob(\"*.js\"):\n",
    "        content = read_and_parse_file(path)\n",
    "        data_list.append(content)\n",
    "    return data_list\n",
    "def convert_data_to_dataframe(data):\n",
    "    dataframe_list = []\n",
    "    for item in data:\n",
    "        df = pd.DataFrame({\n",
    "            \"player_id\": item[\"PlayerID\"],\n",
    "            \"team_id\": item[\"TeamID\"],\n",
    "            \"team_image\": item['TeamImage'],\n",
    "            \"player_image\": item['PlayerImage'],\n",
    "            \"team_code\": item['TeamCode'],\n",
    "            \"team_name\": item[\"TeamName\"],\n",
    "            \"player_name\": item[\"PlayerName\"],\n",
    "            \"player_short_name\": item[\"PlayerShortName\"],\n",
    "            \"player_skill\": item[\"PlayerSkill\"],\n",
    "            \"batting_type\": item[\"BattingType\"],\n",
    "            \"bowling_proficiency\": item[\"BowlingProficiency\"],\n",
    "            \"is_captain\": item[\"IsCaptain\"],\n",
    "            \"is_vice_captain\": item[\"IsViceCaptain\"]\n",
    "        }, index=range(1))\n",
    "        dataframe_list.append(df)\n",
    "    return pd.concat(dataframe_list)\n",
    "def convert_data_list(data_list):\n",
    "    dataframe_list = []\n",
    "    for json_data in data_list:\n",
    "        squad_a_dataframe = convert_data_to_dataframe(json_data[\"squadA\"])\n",
    "        squad_b_dataframe = convert_data_to_dataframe(json_data[\"squadB\"])\n",
    "        dataframe_list.append(pd.concat([squad_a_dataframe, squad_b_dataframe]))\n",
    "    return pd.concat(dataframe_list)\n",
    "def convert_null_values(dataframe):\n",
    "    # Check for null values in the dataframe\n",
    "    null_values = dataframe.isnull()\n",
    "    \n",
    "    # Iterate over each column\n",
    "    for column in dataframe.columns:\n",
    "        # Check if the column contains null values\n",
    "        if null_values[column].any():\n",
    "            # Check if the column is of integer type\n",
    "            if dataframe[column].dtype == 'int64':\n",
    "                # Convert integer null values to -1\n",
    "                dataframe[column] = dataframe[column].fillna(-1)\n",
    "            else:\n",
    "                # Convert character null values to \"NA\"\n",
    "                dataframe[column] = dataframe[column].fillna(\"NA\")\n",
    "    \n",
    "    return dataframe\n",
    "def convert_data_to_df(data_list):\n",
    "    dataframe_list = []\n",
    "    for json_data in data_list:\n",
    "        squad_a_dataframe = convert_data_to_dataframe(json_data[\"squadA\"])\n",
    "        squad_b_dataframe = convert_data_to_dataframe(json_data[\"squadB\"])\n",
    "        dataframe_list.append(pd.concat([squad_a_dataframe, squad_b_dataframe]))\n",
    "    return dataframe_list\n",
    "def update_players_data(new_players_df_list, existing_players_df):    \n",
    "    # Measure execution time for the entire function\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, new_players in enumerate(new_players_df_list):\n",
    "        if i != 0:\n",
    "            existing_players_df = updated_players\n",
    "        \n",
    "        new_players.set_index('player_id', inplace=True)\n",
    "\n",
    "        # Measure execution time for updating existing_players_df with new_players df\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        existing_players_ids = set(existing_players_df.index)\n",
    "        new_players_ids = set(new_players.index)\n",
    "        common_ids = existing_players_ids.intersection(new_players_ids)\n",
    "\n",
    "        existing_players_common_mask = existing_players_df.index.isin(common_ids)\n",
    "        new_players_common_mask = new_players.index.isin(common_ids)\n",
    "\n",
    "        update_columns = ['team_id', 'team_name', 'team_code', 'team_image', 'is_captain', 'is_vice_captain']\n",
    "\n",
    "        existing_players_update_dataframe = existing_players_df.loc[existing_players_common_mask, update_columns]\n",
    "        new_players_update_dataframe = new_players.loc[new_players_common_mask, update_columns]\n",
    "\n",
    "        existing_players_update_dataframe.update(new_players_update_dataframe)\n",
    "\n",
    "        updated_players = pd.concat([existing_players_df, new_players.loc[~new_players_common_mask]])\n",
    "\n",
    "        # Measure execution time for updating existing_players_df\n",
    "        update_end_time = time.time()\n",
    "        update_time = update_end_time - update_start_time\n",
    "        # print(f\"Update time for iteration {i}: {update_time} seconds\")\n",
    "\n",
    "    # Measure execution time for the entire function\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    # print(f\"Total execution time: {total_time} seconds\")\n",
    "\n",
    "    return updated_players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ load initial data \n",
    "init_players = convert_data_list(read_json_files(init_data_path))\n",
    "\n",
    "########### create new delta table in delta lake and add initial data into that \n",
    "write_deltalake(delta_lake_path,init_players,overwrite_schema=True,mode='overwrite',storage_options=options) ## neither overwrite or append mode \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## read existing player's data as delta table\n",
    "existing_players_delta_table = DeltaTable(delta_lake_path,storage_options=options)\n",
    "\n",
    "############## get delta table schema \n",
    "existing_players_delta_table_schema = existing_players_delta_table.schema()\n",
    "\n",
    "############ convert delta table to dataframe and set index\n",
    "existing_players_df = existing_players_delta_table.to_pandas()\n",
    "existing_players_df.set_index('player_id',inplace=True)\n",
    "\n",
    "print(f'{len(existing_players_df)} players are already in dl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_players_df_list = convert_data_to_df(read_json_files(main_data_path))\n",
    "print(f'players from {len(new_players_df_list)} matches need to get updated in dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_players_df = update_players_data(new_players_df_list,existing_players_df)\n",
    "print(f'now there are {len(updated_players_df)} players')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### write updated player's data to delta lake ############\n",
    "\n",
    "write_deltalake(delta_lake_path,updated_players,mode='overwrite',storage_options=options,overwrite_schema=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
